{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13c3d035-e085-44d8-8b5d-165f81bf188a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration charg√©e. Device: cpu\n",
      "üìÇ R√©sultats: /home/ulysse/Bureau/CD LAB/IRM/WORK/results_3d\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# === CHEMINS (A ADAPTER SI BESOIN) ===\n",
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "# Dossiers de donn√©es\n",
    "IMAGES_DIR = os.path.join(BASE_DIR, \"data\", \"dataset_continuous_lesions\", \"image\")\n",
    "MASKS_DIR = os.path.join(BASE_DIR, \"data\", \"dataset_continuous_lesions\", \"mask\")\n",
    "\n",
    "# Dossiers Code et Poids\n",
    "SEGDINO_REPO = os.path.join(BASE_DIR, \"SegDINO\")\n",
    "DINOV3_REPO = os.path.join(BASE_DIR, \"dinov3\")\n",
    "# Attention: V√©rifie bien le nom exact du fichier .pth\n",
    "WEIGHTS_PATH = os.path.join(BASE_DIR, \"weights\", \"dinov3_vits16_pretrain_lvd1689m-08c60483.pth\")\n",
    "\n",
    "# Dossier de sortie\n",
    "RESULTS_DIR = os.path.join(BASE_DIR, \"results_3d\")\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# === HYPERPARAM√àTRES ===\n",
    "#DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE = torch.device('cpu')\n",
    "BATCH_SIZE = 1  # Obligatoire √† 1 pour le volumique (profondeurs variables)\n",
    "EPOCHS = 20\n",
    "LR = 2e-4\n",
    "IMG_SIZE = 224\n",
    "\n",
    "print(f\"‚úÖ Configuration charg√©e. Device: {DEVICE}\")\n",
    "print(f\"üìÇ R√©sultats: {RESULTS_DIR}\")\n",
    "\n",
    "# Ajout des repos au path\n",
    "if SEGDINO_REPO not in sys.path: sys.path.append(SEGDINO_REPO)\n",
    "if DINOV3_REPO not in sys.path: sys.path.append(DINOV3_REPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "369dfffd-0bb4-4914-91fd-0964439cf945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset: 687 s√©quences volumiques trouv√©es.\n"
     ]
    }
   ],
   "source": [
    "class PatientVolumetricDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.volume_groups = {}\n",
    "        all_files = sorted([f for f in os.listdir(img_dir) if f.endswith('.png')])\n",
    "        \n",
    "        # --- PARSING DU NOMMAGE ---\n",
    "        # Format attendu: Patient_Axe_SliceGlobale_SeqID_SeqPos.png\n",
    "        for f in all_files:\n",
    "            try:\n",
    "                # On retire l'extension .png pour le split\n",
    "                parts = f.replace('.png', '').split('_')\n",
    "                if len(parts) >= 5:\n",
    "                    # Cl√© unique = Patient + Axe + SeqID\n",
    "                    # parts[0]: sub-strokecaseXXX\n",
    "                    # parts[1]: axial/coronal/sagittal\n",
    "                    # parts[3]: seqXX\n",
    "                    unique_group_key = f\"{parts[0]}_{parts[1]}_{parts[3]}\"\n",
    "                    \n",
    "                    if unique_group_key not in self.volume_groups:\n",
    "                        self.volume_groups[unique_group_key] = []\n",
    "                    self.volume_groups[unique_group_key].append(f)\n",
    "            except Exception as e:\n",
    "                pass # Fichier mal nomm√© ignor√©\n",
    "                \n",
    "        self.group_ids = list(self.volume_groups.keys())\n",
    "        print(f\"‚úÖ Dataset: {len(self.group_ids)} s√©quences volumiques trouv√©es.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.group_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        group_key = self.group_ids[idx]\n",
    "        files = self.volume_groups[group_key]\n",
    "        \n",
    "        # TRI IMPORTANT : On trie par le num√©ro de position dans la s√©quence (le dernier chiffre)\n",
    "        # Ex: ..._01.png, ..._02.png\n",
    "        files.sort(key=lambda x: int(x.replace('.png', '').split('_')[-1]))\n",
    "        \n",
    "        imgs = []\n",
    "        masks = []\n",
    "        \n",
    "        for f_name in files:\n",
    "            img_path = os.path.join(self.img_dir, f_name)\n",
    "            mask_path = os.path.join(self.mask_dir, f_name)\n",
    "            \n",
    "            # Image\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            imgs.append(img)\n",
    "            \n",
    "            # Masque\n",
    "            mask = Image.open(mask_path).convert(\"L\")\n",
    "            mask = mask.resize((IMG_SIZE, IMG_SIZE), Image.NEAREST)\n",
    "            mask_np = np.array(mask)\n",
    "            # Masque binaire (0 ou 1)\n",
    "            mask_tensor = torch.from_numpy(mask_np > 0).float().unsqueeze(0)\n",
    "            masks.append(mask_tensor)\n",
    "        \n",
    "        # Stack pour cr√©er le volume (Depth, C, H, W)\n",
    "        volume_img = torch.stack(imgs, dim=0)\n",
    "        volume_mask = torch.stack(masks, dim=0)\n",
    "        \n",
    "        return volume_img, volume_mask\n",
    "\n",
    "# Transformations DINO standards\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Instanciation et Split\n",
    "full_ds = PatientVolumetricDataset(IMAGES_DIR, MASKS_DIR, transform=img_transforms)\n",
    "train_size = int(0.8 * len(full_ds))\n",
    "val_size = len(full_ds) - train_size\n",
    "train_ds, val_ds = random_split(full_ds, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e128f9ee-a312-4da6-bca3-c33cc3c3ecb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Classe DPT (SegDINO) import√©e.\n",
      "üèóÔ∏è Chargement du backbone...\n",
      "‚úÖ Backbone charg√©.\n",
      "‚úÖ Mod√®le de base 2D initialis√© (pour extraction des composants).\n"
     ]
    }
   ],
   "source": [
    "# Import de la classe officielle SegDINO\n",
    "try:\n",
    "    from dpt import DPT\n",
    "    print(\"‚úÖ Classe DPT (SegDINO) import√©e.\")\n",
    "except ImportError:\n",
    "    raise ImportError(\"Impossible d'importer dpt.py. V√©rifiez le chemin SEGDINO_REPO.\")\n",
    "\n",
    "# Chargement du Backbone DINOv3 (Local)\n",
    "print(\"üèóÔ∏è Chargement du backbone...\")\n",
    "try:\n",
    "    backbone = torch.hub.load(\n",
    "        repo_or_dir=DINOV3_REPO, \n",
    "        model='dinov3_vits16',  # Version Small\n",
    "        source='local', \n",
    "        weights=WEIGHTS_PATH\n",
    "    )\n",
    "    print(\"‚úÖ Backbone charg√©.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Erreur chargement DINOv3: {e}\")\n",
    "\n",
    "# Instanciation du mod√®le SegDINO de base (2D) pour r√©cup√©rer ses composants\n",
    "base_model_2d = DPT(nclass=1, backbone=backbone).to(DEVICE)\n",
    "print(\"‚úÖ Mod√®le de base 2D initialis√© (pour extraction des composants).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02872241-3917-4b22-a169-225cafbf86df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le 3D pr√™t (Dimension corrig√©e).\n"
     ]
    }
   ],
   "source": [
    "class SegDINO3D(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.backbone = base_model.backbone\n",
    "        self.head = base_model.head\n",
    "        self.encoder_size = base_model.encoder_size\n",
    "        self.intermediate_layer_idx = base_model.intermediate_layer_idx\n",
    "        \n",
    "        # Param√®tres dimensionnels (ViT Small = 384)\n",
    "        self.embed_dim = self.backbone.embed_dim \n",
    "        self.total_dim = self.embed_dim * 4 # 1536\n",
    "        \n",
    "        # --- MODULE DE FUSION 3D ---\n",
    "        self.mixer_3d = nn.Sequential(\n",
    "            nn.Conv3d(self.total_dim, self.total_dim, kernel_size=(3, 1, 1), padding=(1, 0, 0), groups=self.total_dim),\n",
    "            nn.InstanceNorm3d(self.total_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv3d(self.total_dim, self.total_dim, kernel_size=1),\n",
    "            nn.InstanceNorm3d(self.total_dim),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        nn.init.zeros_(self.mixer_3d[-3].weight)\n",
    "        nn.init.zeros_(self.mixer_3d[-3].bias)\n",
    "\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x_volume):\n",
    "        # Input: (Batch=1, Depth, 3, H, W)\n",
    "        b, depth, c, h, w = x_volume.shape\n",
    "        \n",
    "        # 1. Extraction Features 2D\n",
    "        x_flat = x_volume.view(b * depth, c, h, w)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.backbone.get_intermediate_layers(\n",
    "                x_flat, \n",
    "                n=self.intermediate_layer_idx[self.encoder_size]\n",
    "            )\n",
    "        \n",
    "        # 2. Reconstitution des features multi-niveaux\n",
    "        features_list = []\n",
    "        patch_h, patch_w = h // 16, w // 16 \n",
    "        num_tokens = patch_h * patch_w\n",
    "        \n",
    "        for feat in features:\n",
    "            # feat: (TotalImages, N_tokens, Dim) -> (TotalImages, Dim, 14, 14)\n",
    "            feat_spatial = feat[:, -num_tokens:, :] \n",
    "            feat_img = feat_spatial.permute(0, 2, 1).reshape(depth, self.embed_dim, patch_h, patch_w)\n",
    "            features_list.append(feat_img)\n",
    "            \n",
    "        # Concat√©nation -> (Depth, 1536, 14, 14)\n",
    "        concat_feat = torch.cat(features_list, dim=1)\n",
    "        \n",
    "        # 3. FUSION 3D\n",
    "        # On passe en (Batch, Channels, Depth, H, W)\n",
    "        # Avant: (1, Depth, Channels, H, W)\n",
    "        # Apr√®s permute(0, 2, 1, 3, 4): (1, Channels, Depth, H, W) <--- CORRECTION ICI\n",
    "        feat_3d_in = concat_feat.unsqueeze(0).permute(0, 2, 1, 3, 4)\n",
    "        \n",
    "        # Application du m√©langeur 3D + Connexion r√©siduelle\n",
    "        feat_3d_out = self.mixer_3d(feat_3d_in) + feat_3d_in\n",
    "        \n",
    "        # Retour au format pour le MLP (Depth, Channels, H, W)\n",
    "        # (1, C, D, H, W) -> squeeze -> (C, D, H, W) -> permute -> (D, C, H, W)\n",
    "        feat_2d_ready = feat_3d_out.squeeze(0).permute(1, 0, 2, 3)\n",
    "        \n",
    "        # 4. PR√âPARATION POUR LE D√âCODEUR OFFICIEL\n",
    "        split_features = torch.chunk(feat_2d_ready, 4, dim=1)\n",
    "        \n",
    "        final_input_list = []\n",
    "        for feat in split_features:\n",
    "            feat_flat = feat.flatten(2).permute(0, 2, 1)\n",
    "            final_input_list.append(feat_flat)\n",
    "            \n",
    "        # 5. D√©codage Final (MLP)\n",
    "        logits = self.head(final_input_list, patch_h, patch_w)\n",
    "        \n",
    "        mask = F.interpolate(logits, size=(h, w), mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # Output: (1, Depth, 1, H, W)\n",
    "        return mask.unsqueeze(0).permute(0, 1, 2, 3, 4)\n",
    "\n",
    "# Instanciation\n",
    "model = SegDINO3D(base_model_2d).to(DEVICE)\n",
    "print(\"‚úÖ Mod√®le 3D pr√™t (Dimension corrig√©e).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "902d264d-851b-4be1-8809-a285bfcfe328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51602/1054588429.py:32: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler() # Pour √©conomiser la m√©moire GPU\n"
     ]
    }
   ],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = torch.sigmoid(inputs).view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        inter = (inputs * targets).sum()\n",
    "        dice = (2. * inter + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "    def forward(self, inputs, targets):\n",
    "        return 0.5 * self.bce(inputs, targets) + 0.5 * self.dice(inputs, targets)\n",
    "\n",
    "def compute_metrics(logits, targets):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs > 0.5).float().view(-1)\n",
    "    targs = targets.view(-1)\n",
    "    inter = (preds * targs).sum()\n",
    "    union = preds.sum() + targs.sum()\n",
    "    dice = (2. * inter + 1e-6) / (union + 1e-6)\n",
    "    return dice.item()\n",
    "\n",
    "criterion = CombinedLoss()\n",
    "# On entra√Æne uniquement les parties non-gel√©es (Mixer 3D + Decoder)\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "scaler = GradScaler() # Pour √©conomiser la m√©moire GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48286f5-b7da-484b-aab2-8ea9e57e4fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ D√©marrage Entra√Ænement 3D (Batch=1, 20 Epochs)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|                                          | 0/549 [00:00<?, ?it/s]/tmp/ipykernel_51602/2137659684.py:18: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 549/549 [09:00<00:00,  1.02it/s, loss=0.5234]\n",
      "/tmp/ipykernel_51602/2137659684.py:39: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.3426 | Val Dice: 0.5130\n",
      "  üíæ Nouveau meilleur mod√®le sauvegard√© !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 549/549 [10:29<00:00,  1.15s/it, loss=0.2153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 0.2751 | Val Dice: 0.5389\n",
      "  üíæ Nouveau meilleur mod√®le sauvegard√© !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 549/549 [10:32<00:00,  1.15s/it, loss=0.1898]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.2505 | Val Dice: 0.5900\n",
      "  üíæ Nouveau meilleur mod√®le sauvegard√© !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:  19%|‚ñà‚ñà‚ñà‚ñã               | 105/549 [01:39<06:40,  1.11it/s, loss=0.5015]"
     ]
    }
   ],
   "source": [
    "history = {'train_loss': [], 'val_dice': []}\n",
    "best_dice = 0.0\n",
    "\n",
    "print(f\"üöÄ D√©marrage Entra√Ænement 3D (Batch=1, {EPOCHS} Epochs)...\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # --- TRAIN ---\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for vol_img, vol_mask in pbar:\n",
    "        vol_img, vol_mask = vol_img.to(DEVICE), vol_mask.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed Precision\n",
    "        with autocast():\n",
    "            outputs = model(vol_img)\n",
    "            loss = criterion(outputs, vol_mask)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    \n",
    "    # --- VALIDATION ---\n",
    "    model.eval()\n",
    "    val_dice = 0.0\n",
    "    with torch.no_grad():\n",
    "        for vol_img, vol_mask in val_loader:\n",
    "            vol_img = vol_img.to(DEVICE)\n",
    "            vol_mask = vol_mask.to(DEVICE)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(vol_img)\n",
    "            \n",
    "            val_dice += compute_metrics(outputs, vol_mask)\n",
    "            \n",
    "    avg_dice = val_dice / len(val_loader)\n",
    "    \n",
    "    history['train_loss'].append(avg_loss)\n",
    "    history['val_dice'].append(avg_dice)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | Val Dice: {avg_dice:.4f}\")\n",
    "    \n",
    "    if avg_dice > best_dice:\n",
    "        best_dice = avg_dice\n",
    "        torch.save(model.state_dict(), os.path.join(RESULTS_DIR, \"best_segdino_3d.pth\"))\n",
    "        print(\"  üíæ Nouveau meilleur mod√®le sauvegard√© !\")\n",
    "\n",
    "print(\"üèÅ Termin√©.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "944f3d42-512c-4cf4-b65e-5fd081a8baa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "DinoVisionTransformer.forward_features() got an unexpected keyword argument 'pixel_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m vol_img = vol_img.to(DEVICE)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvol_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     preds = (torch.sigmoid(outputs) > \u001b[32m0.5\u001b[39m).float().cpu()\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Afficher la slice du milieu\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DINOV3/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DINOV3/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mSegDINO3D.forward\u001b[39m\u001b[34m(self, x_volume)\u001b[39m\n\u001b[32m     39\u001b[39m x_flat = x_volume.view(b * depth, c, h, w)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad(): \u001b[38;5;66;03m# Pas de gradients pour DINO\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx_flat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     hidden_states = outputs.hidden_states\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# 2. Reconstitution des features multi-niveaux\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DINOV3/lib/python3.11/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/DINOV3/lib/python3.11/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Bureau/CD LAB/IRM/WORK/dinov3/dinov3/models/vision_transformer.py:325\u001b[39m, in \u001b[36mDinoVisionTransformer.forward\u001b[39m\u001b[34m(self, is_training, *args, **kwargs)\u001b[39m\n\u001b[32m    324\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, is_training: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs) -> List[Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]] | Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[32m    327\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[31mTypeError\u001b[39m: DinoVisionTransformer.forward_features() got an unexpected keyword argument 'pixel_values'"
     ]
    }
   ],
   "source": [
    "# Courbes\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Loss')\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['val_dice'], color='green', label='Dice')\n",
    "plt.title(\"Validation Dice (3D)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Exemple visuel\n",
    "model.eval()\n",
    "vol_img, vol_mask = next(iter(val_loader))\n",
    "vol_img = vol_img.to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(vol_img)\n",
    "    preds = (torch.sigmoid(outputs) > 0.5).float().cpu()\n",
    "\n",
    "# Afficher la slice du milieu\n",
    "depth = vol_img.shape[1]\n",
    "mid = depth // 2\n",
    "\n",
    "img_show = vol_img[0, mid].cpu().permute(1, 2, 0).numpy()\n",
    "# Normalisation pour affichage\n",
    "img_show = (img_show - img_show.min()) / (img_show.max() - img_show.min())\n",
    "\n",
    "mask_show = vol_mask[0, mid, 0].cpu().numpy()\n",
    "pred_show = preds[0, mid, 0].numpy()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax[0].imshow(img_show)\n",
    "ax[0].set_title(\"Input (Slice Milieu)\")\n",
    "ax[1].imshow(mask_show, cmap='gray')\n",
    "ax[1].set_title(\"V√©rit√©\")\n",
    "ax[2].imshow(pred_show, cmap='gray')\n",
    "ax[2].set_title(\"Pr√©diction 3D\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c864c30-0726-49f5-a4ee-bce893de805e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DINOV3)",
   "language": "python",
   "name": "dinov3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
